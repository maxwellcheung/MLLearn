{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4424e230",
   "metadata": {},
   "source": [
    "#### 线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e314ce",
   "metadata": {},
   "source": [
    "##### 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10aef192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[0.09150346 0.38253303 0.56802796 0.65080838 0.69838482 0.45911975\n",
      "  0.81531773 0.10825329 0.32132788 0.65849552]\n",
      " [0.34823725 0.52689761 0.76052561 0.811579   0.11093559 0.38922179\n",
      "  0.81909977 0.84519138 0.43731844 0.33471118]]\n",
      "y: [ 3.69663462e+00  3.50398986e+00  5.59184559e+00  4.97493724e+00\n",
      "  3.96939341e+00  3.23792563e+00  5.67313057e+00  6.03525765e+00\n",
      "  4.20998822e+00  4.00753883e+00  6.21259445e+00  5.58509087e+00\n",
      " -1.79304101e-03  7.29437041e+00  5.09061056e+00  6.28512879e+00\n",
      "  2.80010001e+00  7.45869234e+00  1.94071552e+00  4.00514433e+00\n",
      "  3.54019583e+00  3.75516783e+00  6.00561174e+00  7.62568859e+00\n",
      "  4.79979986e+00  9.17358380e+00  6.34191230e+00  7.54555877e+00\n",
      "  5.88254095e+00  5.75928525e+00  1.82670652e+00  6.22472443e+00\n",
      "  7.00252864e+00  6.33119612e+00  4.26640287e+00  2.24156293e+00\n",
      "  2.83210507e+00  2.54247546e+00  5.45860576e+00  4.54260322e+00\n",
      "  2.36256421e+00  8.32969236e+00  4.24480674e+00  5.04100416e+00\n",
      "  5.77213199e+00  5.58086714e+00  8.22814652e+00  6.06430526e+00\n",
      "  6.78632685e+00  6.25592793e+00  7.12579605e+00  6.06162787e+00\n",
      "  4.44244345e+00  4.10726404e+00  6.07366219e+00  2.23814887e+00\n",
      "  3.89542232e+00  5.64106145e+00  6.75503566e+00  7.11125480e+00\n",
      "  2.85156949e+00  4.94951854e+00  4.42820292e+00  6.96888192e+00\n",
      "  6.62079119e+00  7.40145130e+00  3.41816216e+00  4.91922781e+00\n",
      "  3.50124684e+00  4.47878492e+00  1.18230273e+01  2.88083155e+00\n",
      "  5.42628470e+00  6.21656093e+00  8.90199968e-01  2.38828709e+00\n",
      "  5.96358921e+00  5.13495136e+00  4.00045826e+00  5.09294222e+00\n",
      "  6.77449832e+00  6.34898255e+00  6.38748899e+00  1.44182738e-01\n",
      "  7.48518486e+00  1.96401024e+00  5.11269097e+00  6.62303121e+00\n",
      "  7.40381266e+00  4.43798813e+00  1.79505740e+00  1.89581602e+00\n",
      "  4.20842638e+00  3.87859123e+00  5.47271477e+00  7.05297639e+00\n",
      "  5.92550848e+00  3.58434151e+00  4.56182191e+00  2.55113230e+00]\n",
      "y shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "# 构造数据集\n",
    "import numpy as np\n",
    "\n",
    "# 生成一个样本量为100，特征量为10的随机数据集\n",
    "X = np.random.rand(100, 10)\n",
    "print('X:', X[:2, :])  # 打印前2行数据\n",
    "# 构造目标值 y = 3 + 2*X1 +5*X2 - 3*X3\n",
    "y = 3 + 2 * X[:, 0] + 5 * X[:, 1] - 3 * X[:, 2] + np.random.randn(100)\n",
    "print('y:', y)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9375c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class LR():\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    \n",
    "    def get_loss(self, y, y_pre):\n",
    "        loss = np.mean((y - y_pre) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, epochs=500, lr=0.01):\n",
    "        sample, feature = X.shape\n",
    "        if self.w is None:\n",
    "            self.w = np.random.randn(feature)\n",
    "            self.b = 0.0\n",
    "        \n",
    "        # 开始训练\n",
    "        for i in range(epochs):\n",
    "            y_pre = np.dot(X, self.w) + self.b\n",
    "            diff = y_pre - y\n",
    "            dw = 2/sample * np.dot(X.T, diff)\n",
    "            db = 2/sample * np.sum(diff, axis=0)\n",
    "            # 更新参数\n",
    "            self.w -= lr * dw\n",
    "            self.b -= lr * db\n",
    "            # 计算损失\n",
    "            loss = self.get_loss(y, y_pre)\n",
    "            print(f'Epoch {i+1}/{epochs}, Loss: {loss:.4f}')\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "315c0391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 12.8899\n",
      "Epoch 2/500, Loss: 11.7523\n",
      "Epoch 3/500, Loss: 10.7677\n",
      "Epoch 4/500, Loss: 9.9153\n",
      "Epoch 5/500, Loss: 9.1772\n",
      "Epoch 6/500, Loss: 8.5378\n",
      "Epoch 7/500, Loss: 7.9837\n",
      "Epoch 8/500, Loss: 7.5033\n",
      "Epoch 9/500, Loss: 7.0867\n",
      "Epoch 10/500, Loss: 6.7251\n",
      "Epoch 11/500, Loss: 6.4111\n",
      "Epoch 12/500, Loss: 6.1383\n",
      "Epoch 13/500, Loss: 5.9010\n",
      "Epoch 14/500, Loss: 5.6944\n",
      "Epoch 15/500, Loss: 5.5143\n",
      "Epoch 16/500, Loss: 5.3573\n",
      "Epoch 17/500, Loss: 5.2200\n",
      "Epoch 18/500, Loss: 5.0999\n",
      "Epoch 19/500, Loss: 4.9946\n",
      "Epoch 20/500, Loss: 4.9021\n",
      "Epoch 21/500, Loss: 4.8207\n",
      "Epoch 22/500, Loss: 4.7489\n",
      "Epoch 23/500, Loss: 4.6854\n",
      "Epoch 24/500, Loss: 4.6290\n",
      "Epoch 25/500, Loss: 4.5788\n",
      "Epoch 26/500, Loss: 4.5340\n",
      "Epoch 27/500, Loss: 4.4937\n",
      "Epoch 28/500, Loss: 4.4576\n",
      "Epoch 29/500, Loss: 4.4248\n",
      "Epoch 30/500, Loss: 4.3951\n",
      "Epoch 31/500, Loss: 4.3680\n",
      "Epoch 32/500, Loss: 4.3431\n",
      "Epoch 33/500, Loss: 4.3202\n",
      "Epoch 34/500, Loss: 4.2990\n",
      "Epoch 35/500, Loss: 4.2792\n",
      "Epoch 36/500, Loss: 4.2607\n",
      "Epoch 37/500, Loss: 4.2433\n",
      "Epoch 38/500, Loss: 4.2269\n",
      "Epoch 39/500, Loss: 4.2113\n",
      "Epoch 40/500, Loss: 4.1964\n",
      "Epoch 41/500, Loss: 4.1821\n",
      "Epoch 42/500, Loss: 4.1684\n",
      "Epoch 43/500, Loss: 4.1552\n",
      "Epoch 44/500, Loss: 4.1424\n",
      "Epoch 45/500, Loss: 4.1300\n",
      "Epoch 46/500, Loss: 4.1179\n",
      "Epoch 47/500, Loss: 4.1060\n",
      "Epoch 48/500, Loss: 4.0945\n",
      "Epoch 49/500, Loss: 4.0831\n",
      "Epoch 50/500, Loss: 4.0719\n",
      "Epoch 51/500, Loss: 4.0609\n",
      "Epoch 52/500, Loss: 4.0501\n",
      "Epoch 53/500, Loss: 4.0394\n",
      "Epoch 54/500, Loss: 4.0288\n",
      "Epoch 55/500, Loss: 4.0183\n",
      "Epoch 56/500, Loss: 4.0080\n",
      "Epoch 57/500, Loss: 3.9977\n",
      "Epoch 58/500, Loss: 3.9875\n",
      "Epoch 59/500, Loss: 3.9774\n",
      "Epoch 60/500, Loss: 3.9673\n",
      "Epoch 61/500, Loss: 3.9574\n",
      "Epoch 62/500, Loss: 3.9474\n",
      "Epoch 63/500, Loss: 3.9376\n",
      "Epoch 64/500, Loss: 3.9278\n",
      "Epoch 65/500, Loss: 3.9180\n",
      "Epoch 66/500, Loss: 3.9083\n",
      "Epoch 67/500, Loss: 3.8986\n",
      "Epoch 68/500, Loss: 3.8890\n",
      "Epoch 69/500, Loss: 3.8794\n",
      "Epoch 70/500, Loss: 3.8699\n",
      "Epoch 71/500, Loss: 3.8604\n",
      "Epoch 72/500, Loss: 3.8509\n",
      "Epoch 73/500, Loss: 3.8415\n",
      "Epoch 74/500, Loss: 3.8321\n",
      "Epoch 75/500, Loss: 3.8227\n",
      "Epoch 76/500, Loss: 3.8134\n",
      "Epoch 77/500, Loss: 3.8041\n",
      "Epoch 78/500, Loss: 3.7949\n",
      "Epoch 79/500, Loss: 3.7857\n",
      "Epoch 80/500, Loss: 3.7765\n",
      "Epoch 81/500, Loss: 3.7673\n",
      "Epoch 82/500, Loss: 3.7582\n",
      "Epoch 83/500, Loss: 3.7491\n",
      "Epoch 84/500, Loss: 3.7401\n",
      "Epoch 85/500, Loss: 3.7311\n",
      "Epoch 86/500, Loss: 3.7221\n",
      "Epoch 87/500, Loss: 3.7131\n",
      "Epoch 88/500, Loss: 3.7042\n",
      "Epoch 89/500, Loss: 3.6953\n",
      "Epoch 90/500, Loss: 3.6864\n",
      "Epoch 91/500, Loss: 3.6776\n",
      "Epoch 92/500, Loss: 3.6688\n",
      "Epoch 93/500, Loss: 3.6600\n",
      "Epoch 94/500, Loss: 3.6512\n",
      "Epoch 95/500, Loss: 3.6425\n",
      "Epoch 96/500, Loss: 3.6338\n",
      "Epoch 97/500, Loss: 3.6252\n",
      "Epoch 98/500, Loss: 3.6165\n",
      "Epoch 99/500, Loss: 3.6079\n",
      "Epoch 100/500, Loss: 3.5993\n",
      "Epoch 101/500, Loss: 3.5908\n",
      "Epoch 102/500, Loss: 3.5823\n",
      "Epoch 103/500, Loss: 3.5738\n",
      "Epoch 104/500, Loss: 3.5653\n",
      "Epoch 105/500, Loss: 3.5569\n",
      "Epoch 106/500, Loss: 3.5485\n",
      "Epoch 107/500, Loss: 3.5401\n",
      "Epoch 108/500, Loss: 3.5318\n",
      "Epoch 109/500, Loss: 3.5234\n",
      "Epoch 110/500, Loss: 3.5152\n",
      "Epoch 111/500, Loss: 3.5069\n",
      "Epoch 112/500, Loss: 3.4987\n",
      "Epoch 113/500, Loss: 3.4904\n",
      "Epoch 114/500, Loss: 3.4823\n",
      "Epoch 115/500, Loss: 3.4741\n",
      "Epoch 116/500, Loss: 3.4660\n",
      "Epoch 117/500, Loss: 3.4579\n",
      "Epoch 118/500, Loss: 3.4498\n",
      "Epoch 119/500, Loss: 3.4418\n",
      "Epoch 120/500, Loss: 3.4337\n",
      "Epoch 121/500, Loss: 3.4257\n",
      "Epoch 122/500, Loss: 3.4178\n",
      "Epoch 123/500, Loss: 3.4098\n",
      "Epoch 124/500, Loss: 3.4019\n",
      "Epoch 125/500, Loss: 3.3940\n",
      "Epoch 126/500, Loss: 3.3862\n",
      "Epoch 127/500, Loss: 3.3783\n",
      "Epoch 128/500, Loss: 3.3705\n",
      "Epoch 129/500, Loss: 3.3627\n",
      "Epoch 130/500, Loss: 3.3550\n",
      "Epoch 131/500, Loss: 3.3472\n",
      "Epoch 132/500, Loss: 3.3395\n",
      "Epoch 133/500, Loss: 3.3319\n",
      "Epoch 134/500, Loss: 3.3242\n",
      "Epoch 135/500, Loss: 3.3166\n",
      "Epoch 136/500, Loss: 3.3090\n",
      "Epoch 137/500, Loss: 3.3014\n",
      "Epoch 138/500, Loss: 3.2938\n",
      "Epoch 139/500, Loss: 3.2863\n",
      "Epoch 140/500, Loss: 3.2788\n",
      "Epoch 141/500, Loss: 3.2713\n",
      "Epoch 142/500, Loss: 3.2639\n",
      "Epoch 143/500, Loss: 3.2564\n",
      "Epoch 144/500, Loss: 3.2490\n",
      "Epoch 145/500, Loss: 3.2416\n",
      "Epoch 146/500, Loss: 3.2343\n",
      "Epoch 147/500, Loss: 3.2269\n",
      "Epoch 148/500, Loss: 3.2196\n",
      "Epoch 149/500, Loss: 3.2124\n",
      "Epoch 150/500, Loss: 3.2051\n",
      "Epoch 151/500, Loss: 3.1979\n",
      "Epoch 152/500, Loss: 3.1906\n",
      "Epoch 153/500, Loss: 3.1835\n",
      "Epoch 154/500, Loss: 3.1763\n",
      "Epoch 155/500, Loss: 3.1691\n",
      "Epoch 156/500, Loss: 3.1620\n",
      "Epoch 157/500, Loss: 3.1549\n",
      "Epoch 158/500, Loss: 3.1479\n",
      "Epoch 159/500, Loss: 3.1408\n",
      "Epoch 160/500, Loss: 3.1338\n",
      "Epoch 161/500, Loss: 3.1268\n",
      "Epoch 162/500, Loss: 3.1198\n",
      "Epoch 163/500, Loss: 3.1128\n",
      "Epoch 164/500, Loss: 3.1059\n",
      "Epoch 165/500, Loss: 3.0990\n",
      "Epoch 166/500, Loss: 3.0921\n",
      "Epoch 167/500, Loss: 3.0852\n",
      "Epoch 168/500, Loss: 3.0784\n",
      "Epoch 169/500, Loss: 3.0716\n",
      "Epoch 170/500, Loss: 3.0648\n",
      "Epoch 171/500, Loss: 3.0580\n",
      "Epoch 172/500, Loss: 3.0512\n",
      "Epoch 173/500, Loss: 3.0445\n",
      "Epoch 174/500, Loss: 3.0378\n",
      "Epoch 175/500, Loss: 3.0311\n",
      "Epoch 176/500, Loss: 3.0244\n",
      "Epoch 177/500, Loss: 3.0178\n",
      "Epoch 178/500, Loss: 3.0112\n",
      "Epoch 179/500, Loss: 3.0046\n",
      "Epoch 180/500, Loss: 2.9980\n",
      "Epoch 181/500, Loss: 2.9914\n",
      "Epoch 182/500, Loss: 2.9849\n",
      "Epoch 183/500, Loss: 2.9784\n",
      "Epoch 184/500, Loss: 2.9719\n",
      "Epoch 185/500, Loss: 2.9654\n",
      "Epoch 186/500, Loss: 2.9590\n",
      "Epoch 187/500, Loss: 2.9525\n",
      "Epoch 188/500, Loss: 2.9461\n",
      "Epoch 189/500, Loss: 2.9397\n",
      "Epoch 190/500, Loss: 2.9334\n",
      "Epoch 191/500, Loss: 2.9270\n",
      "Epoch 192/500, Loss: 2.9207\n",
      "Epoch 193/500, Loss: 2.9144\n",
      "Epoch 194/500, Loss: 2.9081\n",
      "Epoch 195/500, Loss: 2.9018\n",
      "Epoch 196/500, Loss: 2.8956\n",
      "Epoch 197/500, Loss: 2.8894\n",
      "Epoch 198/500, Loss: 2.8831\n",
      "Epoch 199/500, Loss: 2.8770\n",
      "Epoch 200/500, Loss: 2.8708\n",
      "Epoch 201/500, Loss: 2.8647\n",
      "Epoch 202/500, Loss: 2.8585\n",
      "Epoch 203/500, Loss: 2.8524\n",
      "Epoch 204/500, Loss: 2.8463\n",
      "Epoch 205/500, Loss: 2.8403\n",
      "Epoch 206/500, Loss: 2.8342\n",
      "Epoch 207/500, Loss: 2.8282\n",
      "Epoch 208/500, Loss: 2.8222\n",
      "Epoch 209/500, Loss: 2.8162\n",
      "Epoch 210/500, Loss: 2.8102\n",
      "Epoch 211/500, Loss: 2.8043\n",
      "Epoch 212/500, Loss: 2.7984\n",
      "Epoch 213/500, Loss: 2.7924\n",
      "Epoch 214/500, Loss: 2.7866\n",
      "Epoch 215/500, Loss: 2.7807\n",
      "Epoch 216/500, Loss: 2.7748\n",
      "Epoch 217/500, Loss: 2.7690\n",
      "Epoch 218/500, Loss: 2.7632\n",
      "Epoch 219/500, Loss: 2.7574\n",
      "Epoch 220/500, Loss: 2.7516\n",
      "Epoch 221/500, Loss: 2.7458\n",
      "Epoch 222/500, Loss: 2.7401\n",
      "Epoch 223/500, Loss: 2.7344\n",
      "Epoch 224/500, Loss: 2.7287\n",
      "Epoch 225/500, Loss: 2.7230\n",
      "Epoch 226/500, Loss: 2.7173\n",
      "Epoch 227/500, Loss: 2.7117\n",
      "Epoch 228/500, Loss: 2.7060\n",
      "Epoch 229/500, Loss: 2.7004\n",
      "Epoch 230/500, Loss: 2.6948\n",
      "Epoch 231/500, Loss: 2.6893\n",
      "Epoch 232/500, Loss: 2.6837\n",
      "Epoch 233/500, Loss: 2.6782\n",
      "Epoch 234/500, Loss: 2.6726\n",
      "Epoch 235/500, Loss: 2.6671\n",
      "Epoch 236/500, Loss: 2.6616\n",
      "Epoch 237/500, Loss: 2.6562\n",
      "Epoch 238/500, Loss: 2.6507\n",
      "Epoch 239/500, Loss: 2.6453\n",
      "Epoch 240/500, Loss: 2.6399\n",
      "Epoch 241/500, Loss: 2.6345\n",
      "Epoch 242/500, Loss: 2.6291\n",
      "Epoch 243/500, Loss: 2.6237\n",
      "Epoch 244/500, Loss: 2.6184\n",
      "Epoch 245/500, Loss: 2.6130\n",
      "Epoch 246/500, Loss: 2.6077\n",
      "Epoch 247/500, Loss: 2.6024\n",
      "Epoch 248/500, Loss: 2.5971\n",
      "Epoch 249/500, Loss: 2.5919\n",
      "Epoch 250/500, Loss: 2.5866\n",
      "Epoch 251/500, Loss: 2.5814\n",
      "Epoch 252/500, Loss: 2.5762\n",
      "Epoch 253/500, Loss: 2.5710\n",
      "Epoch 254/500, Loss: 2.5658\n",
      "Epoch 255/500, Loss: 2.5606\n",
      "Epoch 256/500, Loss: 2.5555\n",
      "Epoch 257/500, Loss: 2.5504\n",
      "Epoch 258/500, Loss: 2.5453\n",
      "Epoch 259/500, Loss: 2.5402\n",
      "Epoch 260/500, Loss: 2.5351\n",
      "Epoch 261/500, Loss: 2.5300\n",
      "Epoch 262/500, Loss: 2.5250\n",
      "Epoch 263/500, Loss: 2.5199\n",
      "Epoch 264/500, Loss: 2.5149\n",
      "Epoch 265/500, Loss: 2.5099\n",
      "Epoch 266/500, Loss: 2.5049\n",
      "Epoch 267/500, Loss: 2.4999\n",
      "Epoch 268/500, Loss: 2.4950\n",
      "Epoch 269/500, Loss: 2.4901\n",
      "Epoch 270/500, Loss: 2.4851\n",
      "Epoch 271/500, Loss: 2.4802\n",
      "Epoch 272/500, Loss: 2.4753\n",
      "Epoch 273/500, Loss: 2.4705\n",
      "Epoch 274/500, Loss: 2.4656\n",
      "Epoch 275/500, Loss: 2.4608\n",
      "Epoch 276/500, Loss: 2.4559\n",
      "Epoch 277/500, Loss: 2.4511\n",
      "Epoch 278/500, Loss: 2.4463\n",
      "Epoch 279/500, Loss: 2.4415\n",
      "Epoch 280/500, Loss: 2.4368\n",
      "Epoch 281/500, Loss: 2.4320\n",
      "Epoch 282/500, Loss: 2.4273\n",
      "Epoch 283/500, Loss: 2.4225\n",
      "Epoch 284/500, Loss: 2.4178\n",
      "Epoch 285/500, Loss: 2.4131\n",
      "Epoch 286/500, Loss: 2.4085\n",
      "Epoch 287/500, Loss: 2.4038\n",
      "Epoch 288/500, Loss: 2.3991\n",
      "Epoch 289/500, Loss: 2.3945\n",
      "Epoch 290/500, Loss: 2.3899\n",
      "Epoch 291/500, Loss: 2.3853\n",
      "Epoch 292/500, Loss: 2.3807\n",
      "Epoch 293/500, Loss: 2.3761\n",
      "Epoch 294/500, Loss: 2.3716\n",
      "Epoch 295/500, Loss: 2.3670\n",
      "Epoch 296/500, Loss: 2.3625\n",
      "Epoch 297/500, Loss: 2.3580\n",
      "Epoch 298/500, Loss: 2.3534\n",
      "Epoch 299/500, Loss: 2.3490\n",
      "Epoch 300/500, Loss: 2.3445\n",
      "Epoch 301/500, Loss: 2.3400\n",
      "Epoch 302/500, Loss: 2.3356\n",
      "Epoch 303/500, Loss: 2.3311\n",
      "Epoch 304/500, Loss: 2.3267\n",
      "Epoch 305/500, Loss: 2.3223\n",
      "Epoch 306/500, Loss: 2.3179\n",
      "Epoch 307/500, Loss: 2.3135\n",
      "Epoch 308/500, Loss: 2.3092\n",
      "Epoch 309/500, Loss: 2.3048\n",
      "Epoch 310/500, Loss: 2.3005\n",
      "Epoch 311/500, Loss: 2.2962\n",
      "Epoch 312/500, Loss: 2.2918\n",
      "Epoch 313/500, Loss: 2.2875\n",
      "Epoch 314/500, Loss: 2.2833\n",
      "Epoch 315/500, Loss: 2.2790\n",
      "Epoch 316/500, Loss: 2.2747\n",
      "Epoch 317/500, Loss: 2.2705\n",
      "Epoch 318/500, Loss: 2.2663\n",
      "Epoch 319/500, Loss: 2.2620\n",
      "Epoch 320/500, Loss: 2.2578\n",
      "Epoch 321/500, Loss: 2.2537\n",
      "Epoch 322/500, Loss: 2.2495\n",
      "Epoch 323/500, Loss: 2.2453\n",
      "Epoch 324/500, Loss: 2.2412\n",
      "Epoch 325/500, Loss: 2.2370\n",
      "Epoch 326/500, Loss: 2.2329\n",
      "Epoch 327/500, Loss: 2.2288\n",
      "Epoch 328/500, Loss: 2.2247\n",
      "Epoch 329/500, Loss: 2.2206\n",
      "Epoch 330/500, Loss: 2.2165\n",
      "Epoch 331/500, Loss: 2.2125\n",
      "Epoch 332/500, Loss: 2.2084\n",
      "Epoch 333/500, Loss: 2.2044\n",
      "Epoch 334/500, Loss: 2.2003\n",
      "Epoch 335/500, Loss: 2.1963\n",
      "Epoch 336/500, Loss: 2.1923\n",
      "Epoch 337/500, Loss: 2.1883\n",
      "Epoch 338/500, Loss: 2.1844\n",
      "Epoch 339/500, Loss: 2.1804\n",
      "Epoch 340/500, Loss: 2.1765\n",
      "Epoch 341/500, Loss: 2.1725\n",
      "Epoch 342/500, Loss: 2.1686\n",
      "Epoch 343/500, Loss: 2.1647\n",
      "Epoch 344/500, Loss: 2.1608\n",
      "Epoch 345/500, Loss: 2.1569\n",
      "Epoch 346/500, Loss: 2.1530\n",
      "Epoch 347/500, Loss: 2.1492\n",
      "Epoch 348/500, Loss: 2.1453\n",
      "Epoch 349/500, Loss: 2.1415\n",
      "Epoch 350/500, Loss: 2.1376\n",
      "Epoch 351/500, Loss: 2.1338\n",
      "Epoch 352/500, Loss: 2.1300\n",
      "Epoch 353/500, Loss: 2.1262\n",
      "Epoch 354/500, Loss: 2.1224\n",
      "Epoch 355/500, Loss: 2.1187\n",
      "Epoch 356/500, Loss: 2.1149\n",
      "Epoch 357/500, Loss: 2.1112\n",
      "Epoch 358/500, Loss: 2.1074\n",
      "Epoch 359/500, Loss: 2.1037\n",
      "Epoch 360/500, Loss: 2.1000\n",
      "Epoch 361/500, Loss: 2.0963\n",
      "Epoch 362/500, Loss: 2.0926\n",
      "Epoch 363/500, Loss: 2.0889\n",
      "Epoch 364/500, Loss: 2.0853\n",
      "Epoch 365/500, Loss: 2.0816\n",
      "Epoch 366/500, Loss: 2.0780\n",
      "Epoch 367/500, Loss: 2.0743\n",
      "Epoch 368/500, Loss: 2.0707\n",
      "Epoch 369/500, Loss: 2.0671\n",
      "Epoch 370/500, Loss: 2.0635\n",
      "Epoch 371/500, Loss: 2.0599\n",
      "Epoch 372/500, Loss: 2.0563\n",
      "Epoch 373/500, Loss: 2.0528\n",
      "Epoch 374/500, Loss: 2.0492\n",
      "Epoch 375/500, Loss: 2.0457\n",
      "Epoch 376/500, Loss: 2.0421\n",
      "Epoch 377/500, Loss: 2.0386\n",
      "Epoch 378/500, Loss: 2.0351\n",
      "Epoch 379/500, Loss: 2.0316\n",
      "Epoch 380/500, Loss: 2.0281\n",
      "Epoch 381/500, Loss: 2.0246\n",
      "Epoch 382/500, Loss: 2.0212\n",
      "Epoch 383/500, Loss: 2.0177\n",
      "Epoch 384/500, Loss: 2.0143\n",
      "Epoch 385/500, Loss: 2.0108\n",
      "Epoch 386/500, Loss: 2.0074\n",
      "Epoch 387/500, Loss: 2.0040\n",
      "Epoch 388/500, Loss: 2.0006\n",
      "Epoch 389/500, Loss: 1.9972\n",
      "Epoch 390/500, Loss: 1.9938\n",
      "Epoch 391/500, Loss: 1.9904\n",
      "Epoch 392/500, Loss: 1.9871\n",
      "Epoch 393/500, Loss: 1.9837\n",
      "Epoch 394/500, Loss: 1.9804\n",
      "Epoch 395/500, Loss: 1.9770\n",
      "Epoch 396/500, Loss: 1.9737\n",
      "Epoch 397/500, Loss: 1.9704\n",
      "Epoch 398/500, Loss: 1.9671\n",
      "Epoch 399/500, Loss: 1.9638\n",
      "Epoch 400/500, Loss: 1.9605\n",
      "Epoch 401/500, Loss: 1.9573\n",
      "Epoch 402/500, Loss: 1.9540\n",
      "Epoch 403/500, Loss: 1.9507\n",
      "Epoch 404/500, Loss: 1.9475\n",
      "Epoch 405/500, Loss: 1.9443\n",
      "Epoch 406/500, Loss: 1.9410\n",
      "Epoch 407/500, Loss: 1.9378\n",
      "Epoch 408/500, Loss: 1.9346\n",
      "Epoch 409/500, Loss: 1.9314\n",
      "Epoch 410/500, Loss: 1.9282\n",
      "Epoch 411/500, Loss: 1.9251\n",
      "Epoch 412/500, Loss: 1.9219\n",
      "Epoch 413/500, Loss: 1.9188\n",
      "Epoch 414/500, Loss: 1.9156\n",
      "Epoch 415/500, Loss: 1.9125\n",
      "Epoch 416/500, Loss: 1.9093\n",
      "Epoch 417/500, Loss: 1.9062\n",
      "Epoch 418/500, Loss: 1.9031\n",
      "Epoch 419/500, Loss: 1.9000\n",
      "Epoch 420/500, Loss: 1.8969\n",
      "Epoch 421/500, Loss: 1.8939\n",
      "Epoch 422/500, Loss: 1.8908\n",
      "Epoch 423/500, Loss: 1.8877\n",
      "Epoch 424/500, Loss: 1.8847\n",
      "Epoch 425/500, Loss: 1.8816\n",
      "Epoch 426/500, Loss: 1.8786\n",
      "Epoch 427/500, Loss: 1.8756\n",
      "Epoch 428/500, Loss: 1.8726\n",
      "Epoch 429/500, Loss: 1.8696\n",
      "Epoch 430/500, Loss: 1.8666\n",
      "Epoch 431/500, Loss: 1.8636\n",
      "Epoch 432/500, Loss: 1.8606\n",
      "Epoch 433/500, Loss: 1.8576\n",
      "Epoch 434/500, Loss: 1.8547\n",
      "Epoch 435/500, Loss: 1.8517\n",
      "Epoch 436/500, Loss: 1.8488\n",
      "Epoch 437/500, Loss: 1.8458\n",
      "Epoch 438/500, Loss: 1.8429\n",
      "Epoch 439/500, Loss: 1.8400\n",
      "Epoch 440/500, Loss: 1.8371\n",
      "Epoch 441/500, Loss: 1.8342\n",
      "Epoch 442/500, Loss: 1.8313\n",
      "Epoch 443/500, Loss: 1.8284\n",
      "Epoch 444/500, Loss: 1.8256\n",
      "Epoch 445/500, Loss: 1.8227\n",
      "Epoch 446/500, Loss: 1.8198\n",
      "Epoch 447/500, Loss: 1.8170\n",
      "Epoch 448/500, Loss: 1.8142\n",
      "Epoch 449/500, Loss: 1.8113\n",
      "Epoch 450/500, Loss: 1.8085\n",
      "Epoch 451/500, Loss: 1.8057\n",
      "Epoch 452/500, Loss: 1.8029\n",
      "Epoch 453/500, Loss: 1.8001\n",
      "Epoch 454/500, Loss: 1.7973\n",
      "Epoch 455/500, Loss: 1.7945\n",
      "Epoch 456/500, Loss: 1.7918\n",
      "Epoch 457/500, Loss: 1.7890\n",
      "Epoch 458/500, Loss: 1.7863\n",
      "Epoch 459/500, Loss: 1.7835\n",
      "Epoch 460/500, Loss: 1.7808\n",
      "Epoch 461/500, Loss: 1.7780\n",
      "Epoch 462/500, Loss: 1.7753\n",
      "Epoch 463/500, Loss: 1.7726\n",
      "Epoch 464/500, Loss: 1.7699\n",
      "Epoch 465/500, Loss: 1.7672\n",
      "Epoch 466/500, Loss: 1.7645\n",
      "Epoch 467/500, Loss: 1.7618\n",
      "Epoch 468/500, Loss: 1.7592\n",
      "Epoch 469/500, Loss: 1.7565\n",
      "Epoch 470/500, Loss: 1.7539\n",
      "Epoch 471/500, Loss: 1.7512\n",
      "Epoch 472/500, Loss: 1.7486\n",
      "Epoch 473/500, Loss: 1.7459\n",
      "Epoch 474/500, Loss: 1.7433\n",
      "Epoch 475/500, Loss: 1.7407\n",
      "Epoch 476/500, Loss: 1.7381\n",
      "Epoch 477/500, Loss: 1.7355\n",
      "Epoch 478/500, Loss: 1.7329\n",
      "Epoch 479/500, Loss: 1.7303\n",
      "Epoch 480/500, Loss: 1.7277\n",
      "Epoch 481/500, Loss: 1.7252\n",
      "Epoch 482/500, Loss: 1.7226\n",
      "Epoch 483/500, Loss: 1.7200\n",
      "Epoch 484/500, Loss: 1.7175\n",
      "Epoch 485/500, Loss: 1.7149\n",
      "Epoch 486/500, Loss: 1.7124\n",
      "Epoch 487/500, Loss: 1.7099\n",
      "Epoch 488/500, Loss: 1.7074\n",
      "Epoch 489/500, Loss: 1.7049\n",
      "Epoch 490/500, Loss: 1.7024\n",
      "Epoch 491/500, Loss: 1.6999\n",
      "Epoch 492/500, Loss: 1.6974\n",
      "Epoch 493/500, Loss: 1.6949\n",
      "Epoch 494/500, Loss: 1.6924\n",
      "Epoch 495/500, Loss: 1.6900\n",
      "Epoch 496/500, Loss: 1.6875\n",
      "Epoch 497/500, Loss: 1.6851\n",
      "Epoch 498/500, Loss: 1.6826\n",
      "Epoch 499/500, Loss: 1.6802\n",
      "Epoch 500/500, Loss: 1.6777\n"
     ]
    }
   ],
   "source": [
    "#生成一个样本量为100 特征量为10的数据集\n",
    "sample=1\n",
    "feature=10\n",
    "test_x=np.random.rand(sample,10)\n",
    "#构造目标值 y=3+2*x1+5*x2-3*x3\n",
    "test_y=3+2*test_x[:,0]+5*test_x[:,1]-3*test_x[:,2]+np.random.randn(sample) # 增加一点噪音\n",
    "#np.random.randn 生成满足正态分布的随机数\n",
    "\n",
    "model=LR()\n",
    "model.fit(X,y)\n",
    "y_pre=model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6081a9",
   "metadata": {},
   "source": [
    "##### 使用sklearn包实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5438274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重： [ 2.28761353  5.16264964 -2.66373939 -0.05104705  0.22371288  0.04072163\n",
      " -0.12239478  0.7921249  -0.0543275   0.68380205]\n",
      "截距： 1.9553038376273193\n"
     ]
    }
   ],
   "source": [
    "# 生成样本量为100，特征量为10的数据集\n",
    "X = np.random.rand(100, 10)\n",
    "\n",
    "# 构造目标值\n",
    "y = 3  + 2 * X[:, 0] + 5 * X[:, 1] - 3 * X[:, 2] + np.random.randn(100)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pre = model.predict(test_x)\n",
    "\n",
    "# 显示权重系数\n",
    "weights = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "print('权重：', weights)\n",
    "print('截距：', intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ac629",
   "metadata": {},
   "source": [
    "##### 对数几率回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "549334bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99330715]\n"
     ]
    }
   ],
   "source": [
    "# sigmoid函数\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.array([5])\n",
    "print(sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bda265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造数据集\n",
    "#生成一个样本量为100 特征量为10的数据集\n",
    "sample=100\n",
    "feature=10\n",
    "X=np.random.rand(sample,10)\n",
    "#构造目标值 y=3+2*x1+5*x2-3*x3\n",
    "y=3+2*X[:,0]+5*X[:,1]-3*X[:,2]+np.random.randn(sample) # 增加一点噪音\n",
    "\n",
    "# 构建分类问题\n",
    "mean_ = np.mean(y)\n",
    "# 一半数据低于平均，一半高于平均\n",
    "y = np.where(y>mean_,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bc3cbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.85\n"
     ]
    }
   ],
   "source": [
    "# sklearn包实现\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2025)\n",
    "\n",
    "# 创建逻辑回归模型\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# 训练模型\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "predictions = log_reg.predict(X_test)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('准确率：', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3bde2c",
   "metadata": {},
   "source": [
    "##### 线性判别分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00a11549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda准确率： 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 加载数据集\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 划分训练和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2025)\n",
    "\n",
    "# 创建模型\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# 训练模型\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# 预测测试集\n",
    "predictions = lda.predict(X_test)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('lda准确率：', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f019a",
   "metadata": {},
   "source": [
    "多分类学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73d32c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 加载鸢尾花数据集\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 将数据集划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建逻辑回归模型\n",
    "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# 训练模型\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# 预测测试集\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6853d",
   "metadata": {},
   "source": [
    "##### 类别不平衡问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db7a447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征数据：\n",
      " [[-0.90447949  0.40579743  0.83992346 ...  1.03307436 -1.63732419\n",
      "  -0.66011757]\n",
      " [ 0.4818873   0.6162199   0.4308624  ... -0.75106561  0.31506254\n",
      "   0.38420953]\n",
      " [ 0.75192393  1.79550572  1.55230014 ... -1.37298251 -0.06669316\n",
      "   0.63208385]\n",
      " ...\n",
      " [-0.65617246  0.77519106  1.11666824 ...  0.63356167 -1.50970179\n",
      "  -0.4601161 ]\n",
      " [ 0.6944778   1.57243472  1.34306973 ... -1.24738177 -0.00409399\n",
      "   0.58043839]\n",
      " [-0.57605299  1.07916446  1.40094425 ...  0.46010921 -1.59222741\n",
      "  -0.38836568]]\n",
      "标签数据：\n",
      " [0 2 2 1 1 0 1 0 1 2 1 1 0 0 0 2 2 0 1 2 0 0 1 2 2 1 1 1 1 1 2 0 1 0 0 2 0\n",
      " 0 2 2 2 0 1 1 0 1 0 2 1 1 2 1 2 0 0 1 2 1 2 0 0 2 0 1 2 1 1 1 1 2 1 1 0 0\n",
      " 0 2 0 2 1 2 2 2 2 0 0 2 0 2 1 1 1 0 1 2 0 0 2 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "# 类别不平衡\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=3, random_state=42,n_clusters_per_class=1)\n",
    "print(\"特征数据：\\n\", X)\n",
    "print(\"标签数据：\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc269716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据集类别分布： Counter({1: 900, 0: 100})\n",
      "欠采样后数据集类别分布： Counter({0: 100, 1: 100})\n"
     ]
    }
   ],
   "source": [
    "#欠采样\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "# 创建一个不平衡的数据集\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "\n",
    "# 打印原始数据集的类别分布\n",
    "print(\"原始数据集类别分布：\", Counter(y))\n",
    "\n",
    "# 实例化RandomUnderSampler对象\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# 对数据集进行欠采样\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "# 打印欠采样后的数据集类别分布\n",
    "print(\"欠采样后数据集类别分布：\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a52107ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据集类别分布： Counter({1: 900, 0: 100})\n",
      "过采样后数据集类别分布： Counter({0: 900, 1: 900})\n"
     ]
    }
   ],
   "source": [
    "# 过采样\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "# 创建一个不平衡的数据集\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "\n",
    "# 打印原始数据集的类别分布\n",
    "print(\"原始数据集类别分布：\", Counter(y))\n",
    "\n",
    "# 实例化RandomOverSampler对象\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# 对数据集进行过采样\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# 打印过采样后的数据集类别分布\n",
    "print(\"过采样后数据集类别分布：\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cf50bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.125      0.16666667 0.5       ]\n",
      "类别权重： [0.66666667 0.88888889 2.66666667]\n",
      "0.6666666666666666\n",
      "0.8888888888888888\n",
      "2.6666666666666665\n",
      "逻辑回归模型的准确率： 0.99\n"
     ]
    }
   ],
   "source": [
    "# 阈值移动\n",
    "#分类的时候，当不同类别的样本量差异很大时，很容易影响分类结果，因此要么每个类别的数据量大致相同，要么就要进行校正。\n",
    "#sklearn的做法可以是加权，加权就要涉及到class_weight和sample_weight\n",
    "#当不设置class_weight参数时，默认值是所有类别的权值为1\n",
    "\n",
    "#那么'balanced'的计算方法是什么呢？\n",
    "import numpy as np\n",
    "\n",
    "y = [0,0,0,0,0,0,0,0,1,1,1,1,1,1,2,2]  #标签值，一共16个样本\n",
    "\n",
    "a = np.bincount(y)  # array([8, 6, 2], dtype=int64) 计算每个类别的样本数量\n",
    "aa = 1/a  #倒数 array([0.125     , 0.16666667, 0.5       ])\n",
    "print(aa)\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight \n",
    "y = [0,0,0,0,0,0,0,0,1,1,1,1,1,1,2,2]\n",
    "\n",
    "# 计算类别权重\n",
    "class_weights = compute_class_weight('balanced', classes=[0, 1,2], y=y)\n",
    "print(\"类别权重：\", class_weights) # [0.66666667 0.88888889 2.66666667]\n",
    "\n",
    "\n",
    "#weight_ = n_samples / (n_classes * np.bincount(y))\n",
    "\n",
    "print(16/(3*8))  #输出 0.6666666666666666\n",
    "print(16/(3*6))  #输出 0.8888888888888888\n",
    "print(16/(3*2))  #输出 2.6666666666666665\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 创建一个不平衡的数据集\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建逻辑回归模型，并设置class_weight为'balanced'\n",
    "#class_weight 可以设置为 {0:99:，1:1}\n",
    "logreg = LogisticRegression(class_weight='balanced', solver='liblinear', max_iter=1000)\n",
    "\n",
    "# 训练模型\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# 预测测试集\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"逻辑回归模型的准确率：\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
