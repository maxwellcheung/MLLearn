{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42754213",
   "metadata": {},
   "source": [
    "神经网络构造"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af523c3a",
   "metadata": {},
   "source": [
    "Module 类是 torch.nn 模块里提供的一个模型构造类，是所有神经网络模块的基类，可以继承它来定义我们想要的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f16f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定义 MLP 类重载 Module 类的 __init__ 函数和 forward 函数。\n",
    "分别用于创建模型参数和定义前向计算（正向传播）。\n",
    "MLP 类定义了一个具有两个隐藏层的多层感知机\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        # 初始化父类\n",
    "        super().__init__(**kwargs)\n",
    "        # 定义模型参数\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(256, 10)\n",
    "    \n",
    "    # 定义模型的前向计算\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.act(x)\n",
    "        o = self.output(x)\n",
    "        return o  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b96be8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "# netron 可视化模型\n",
    "X = torch.rand(2,784) # 设置一个随机的输入张量\n",
    "net = MLP() # 实例化模型\n",
    "print(net) # 打印模型\n",
    "net(X) # 前向计算\n",
    "torch.onnx.export(net, X, \"model_mlp.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fcb65e",
   "metadata": {},
   "source": [
    "使用 Module 来自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97bce772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2., -1.,  0.,  1.,  2.])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "不含模型参数的层\n",
    "MyLayer 类通过继承 Module 类自定义一个将输入减掉均值后输出的层，\n",
    "并将层的计算定义在了 forward 函数里。这个层里不含模型参数。\n",
    "\"\"\"\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        # 初始化父类\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    # 定义模型的前向计算\n",
    "    def forward(self, x):\n",
    "        return x - x.mean()\n",
    "    \n",
    "# 实例化模型\n",
    "layer = MyLayer()\n",
    "# 前向计算\n",
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "y = layer(x)\n",
    "print(y) # 打印输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41b103af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyListDense(\n",
      "  (param_list): ParameterList(\n",
      "      (0): Parameter containing: [torch.float32 of size 4x4]\n",
      "      (1): Parameter containing: [torch.float32 of size 4x4]\n",
      "      (2): Parameter containing: [torch.float32 of size 4x4]\n",
      "      (3): Parameter containing: [torch.float32 of size 4x1]\n",
      "  )\n",
      ")\n",
      "MyDictDense(\n",
      "  (param_dict): ParameterDict(\n",
      "      (Linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (Linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "      (Linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "自定义含模型参数的自定义层。其中的模型参数可以通过训练学出\n",
    "Parameter 类其实是 Tensor 的子类，如果一个 Tensor 是 Parameter ，那么它会自动被添加到模型的参数列表里。\n",
    "所以在定义含模型参数的层时，应该将参数定义成 Parameter。\n",
    "除了直接定义成 Parameter 类外，还可以使用类 ParameterList 和 ParameterDict 分别定义参数的列表和字典。\n",
    "\"\"\"\n",
    "class MyListDense(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        # 初始化父类\n",
    "        super().__init__(**kwargs)\n",
    "        # 定义模型参数\n",
    "        #创建一个参数列表\n",
    "        self.param_list = nn.ParameterList()\n",
    "        #创建三个4x4参数，并加入到参数列表中\n",
    "        for i in range(3):\n",
    "            self.param_list.append(nn.Parameter(torch.randn(4, 4)))\n",
    "        #创建一个4x1参数，并加入到参数列表中\n",
    "        self.param_list.append(nn.Parameter(torch.randn(4, 1)))\n",
    "\n",
    "    # 定义模型的前向计算\n",
    "    def forward(self, x):\n",
    "        # 计算参数列表的矩阵乘法\n",
    "        for i in range(len(self.parameters)):\n",
    "            x = torch.mm(x, self.parameters[i])\n",
    "        return x\n",
    "    \n",
    "# 实例化模型\n",
    "layer = MyListDense()\n",
    "print(layer)\n",
    "\n",
    "\n",
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        # 初始化父类\n",
    "        super().__init__(**kwargs)\n",
    "        # 定义模型参数\n",
    "        # 创建一个参数字典\n",
    "        self.param_dict = nn.ParameterDict({\"Linear1\": nn.Parameter(torch.randn(4, 4)),\n",
    "                                            \"Linear2\": nn.Parameter(torch.randn(4, 1))})\n",
    "        self.param_dict[\"Linear3\"] = nn.Parameter(torch.randn(4, 2))\n",
    "\n",
    "    def forward(self, x, choice=\"Linear1\"):\n",
    "        return torch.mm(x, self.param_dict[choice])\n",
    "    \n",
    "# 实例化模型\n",
    "layer = MyDictDense()\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca11f13",
   "metadata": {},
   "source": [
    "常见神经网络层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3faba67",
   "metadata": {},
   "source": [
    "卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfd49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "二维卷积层\n",
    "二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。\n",
    "卷积层的模型参数包括了卷积核和标量偏差。\n",
    "在训练模型时，通常先对卷积核随机初始化，然后不断迭代卷积核和偏差\n",
    "实际应用中常使用PyTorch内置的torch.nn.Conv2d，效率更高且支持更多功能\n",
    "\"\"\"\n",
    "# 例如输入矩阵 \n",
    "# [[1,2],[3,4]]与卷积核 [[−1,0],[0,1]]运算\n",
    "# Y = [[−1*1+0*2+0*3+1*4]] = [[3]]\n",
    "# 输出尺寸\n",
    "# H_out = (H_in + 2*padding - kernel_size_h) / stride + 1\n",
    "# W_out = (W_in + 2*padding - kernel_size_w) / stride + 1\n",
    "# ( p )：填充（Padding）数\n",
    "# ( s )：步长（Stride）\n",
    "\n",
    "\n",
    "# 计算二维卷积（互相关）\n",
    "def coor2d(X, K):\n",
    "    # 获取卷积核的形状\n",
    "    k_h, k_w = K.shape\n",
    "    X, K = X.float(), K.float()\n",
    "    # 获取输出的形状\n",
    "    Y = torch.zeros(X.shape[0] - k_h + 1, X.shape[1] - k_w + 1)\n",
    "    # 进行卷积运算\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + k_h, j:j + k_w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "# 二维卷积层\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size, **kwargs):\n",
    "        # 初始化父类\n",
    "        super().__init__(**kwargs)\n",
    "        # 定义模型参数\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "    \n",
    "    # 定义模型的前向计算\n",
    "    def forward(self, x):\n",
    "        return coor2d(x, self.weight) + self.bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ede6bd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "创建一个⾼和宽为3的二维卷积层，\n",
    "设输⼊高和宽两侧的填充数分别为1。\n",
    "给定一个高和宽为8的输入，输出的高和宽也是8\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义函数计算二维卷积层，对输入和输出做相应升维和降维\n",
    "def com_conv2d(conv2d, X):\n",
    "    # 升维\n",
    "    # （1，1，h，w）表示一个批量大小为1，通道数为1的输入\n",
    "    X = X.view((1, 1) + X.shape)\n",
    "    # 计算卷积\n",
    "    Y = conv2d(X)\n",
    "    # 降维\n",
    "    # 排除不关心的前两维:批量和通道\n",
    "    Y = Y.view(Y.shape[2:])\n",
    "    return Y\n",
    "\n",
    "# 实例化卷积层\n",
    "# 注意这里是两侧分别填充1⾏或列，所以在两侧一共填充2⾏或列\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(3, 3), padding=1)\n",
    "X = torch.randn(8, 8)\n",
    "print(com_conv2d(conv2d, X).shape) # 输出卷积后的形状\n",
    "\n",
    "# 当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽\n",
    "# 使用高为5、宽为3的卷积核。在⾼和宽两侧的填充数分别为2和1\n",
    "conv2d_1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(5, 3), padding=(2, 1))\n",
    "X = torch.randn(8, 8)\n",
    "print(com_conv2d(conv2d_1, X).shape) # 输出卷积后的形状\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1e9a1",
   "metadata": {},
   "source": [
    "池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56f10c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.],\n",
      "        [7., 8.]])\n",
      "tensor([[2., 3.],\n",
      "        [5., 6.]])\n",
      "torch.Size([1, 1, 3, 3])\n",
      "tensor([[4., 5.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "池化层每次对输入数据的一个固定形状窗口(⼜称池化窗口)中的元素计算输出。\n",
    "不同于卷积层里计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的属性（均值、最大值等）。\n",
    "常见的池化包括最大池化或平均池化。\n",
    "在二维最大池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。\n",
    "当池化窗口滑动到某⼀位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义函数计算二维池化层\n",
    "def pool2d(X, pool_size, mode='max'):\n",
    "    # 获取池化窗口的形状\n",
    "    p_h, p_w = pool_size\n",
    "    # 获取输出的形状\n",
    "    Y = torch.zeros((X.shape[0] - p_h) + 1, (X.shape[1] - p_w) + 1)\n",
    "    # 进行池化运算\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                # 计算池化窗口内的最大值\n",
    "                # X[i: i + p_h, j: j + p_w]表示池化窗口内的元素\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y\n",
    "\n",
    "# 实例化池化层\n",
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]], dtype=torch.float)\n",
    "print(pool2d(X, (2, 2)))\n",
    "print(pool2d(X, (2, 2), mode='avg')) # 平均池化\n",
    "\n",
    "# 使用PyTorch内置的最大池化\n",
    "# PyTorch池化层要求输入为 4D张量：格式：(batch_size, channels, height, width)\n",
    "#input_4d = X.unsqueeze(0).unsqueeze(0)\n",
    "input_4d = X.view(1, 1, 3, 3) # 升维\n",
    "print(input_4d.shape)\n",
    "max_pool = nn.MaxPool2d(kernel_size=(2, 2), stride=1) # 使用PyTorch内置的最大池化\n",
    "max_output = max_pool(input_4d) # 计算池化\n",
    "print(max_output.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077e413",
   "metadata": {},
   "source": [
    "LeNet模型示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4673081",
   "metadata": {},
   "source": [
    "原始LeNet-5结构（论文版本）：\n",
    "Input (32×32 Grayscale)\n",
    "→Conv6 (5×5)→C1: 28×28×6\n",
    "→AvgPool→S2: 14×14×6\n",
    "→Conv16 (5×5)→C3: 10×10×16\n",
    "→AvgPool→S4: 5×5×16\n",
    "→Conv120 (5×5)→C5: 1×1×120\n",
    "→FC→F6: 84→Output→10 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3721491f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_lenet(\n",
      "  (feature): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): Tanh()\n",
      "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (4): Tanh()\n",
      "    (5): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0145,  0.1264, -0.1493,  0.0898,  0.0716],\n",
      "          [ 0.0741, -0.0731, -0.0322,  0.0336,  0.1859],\n",
      "          [-0.0681, -0.0216, -0.0402,  0.1715,  0.1574],\n",
      "          [-0.0004, -0.0124,  0.1425, -0.0507,  0.1439],\n",
      "          [-0.0354,  0.1988,  0.1777, -0.0116,  0.1631]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1773,  0.1258,  0.1941,  0.1336,  0.0020],\n",
      "          [ 0.0519, -0.0488, -0.0347, -0.0185,  0.1215],\n",
      "          [ 0.1530, -0.0573,  0.1124,  0.1477, -0.0834],\n",
      "          [-0.1860,  0.0411, -0.1292, -0.1799, -0.0683],\n",
      "          [-0.1352,  0.0224, -0.0828,  0.0040,  0.1666]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1662, -0.1764, -0.0591, -0.0388, -0.1900],\n",
      "          [-0.0113,  0.0248, -0.1558, -0.1731,  0.0744],\n",
      "          [-0.1790, -0.0847,  0.1799, -0.0295,  0.0104],\n",
      "          [-0.1783,  0.0076, -0.1004, -0.0078, -0.0471],\n",
      "          [-0.1513,  0.1828, -0.0441,  0.0627,  0.0992]]],\n",
      "\n",
      "\n",
      "        [[[-0.1299, -0.1583,  0.1792, -0.1380, -0.0297],\n",
      "          [-0.0013, -0.1126, -0.0362, -0.1993,  0.0032],\n",
      "          [ 0.1036,  0.0242, -0.0072, -0.1821, -0.0712],\n",
      "          [-0.1500, -0.1340,  0.1381, -0.1802, -0.0842],\n",
      "          [ 0.0638,  0.1625,  0.1979, -0.0517, -0.0390]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0830, -0.1652,  0.0213,  0.0693,  0.0535],\n",
      "          [ 0.0484, -0.1834, -0.1953, -0.1076, -0.1167],\n",
      "          [ 0.0230,  0.1049,  0.0050,  0.0462, -0.1295],\n",
      "          [-0.0576,  0.1110, -0.1798, -0.1409, -0.1020],\n",
      "          [ 0.0055, -0.0984,  0.1480,  0.0614, -0.1365]]],\n",
      "\n",
      "\n",
      "        [[[-0.0315, -0.0990,  0.1449, -0.0028, -0.1273],\n",
      "          [-0.1471, -0.1070,  0.0886, -0.1387, -0.1915],\n",
      "          [ 0.0234,  0.0773, -0.0402, -0.1021, -0.1444],\n",
      "          [ 0.1776, -0.0988,  0.1820, -0.0564, -0.0944],\n",
      "          [-0.1288,  0.1674,  0.1390, -0.1911,  0.1221]]]], requires_grad=True)\n",
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class my_lenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # nn.Sequential 是PyTorch中用于构建顺序执行模型结构的容器类\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5), # 卷积层\n",
    "            nn.Tanh(), # 激活函数\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2), # 平均池化层\n",
    "            nn.Conv2d(6, 16, kernel_size=5), # 卷积层\n",
    "            nn.Tanh(), # 激活函数\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2), # 平均池化层\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), # 展平层\n",
    "            nn.Linear(16 * 5 * 5, 120), # 全连接层\n",
    "            nn.Tanh(), # 激活函数\n",
    "            nn.Linear(120, 84), # 全连接层\n",
    "            nn.Tanh(), # 激活函数\n",
    "            nn.Linear(84, 10) # 全连接层\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播\n",
    "        x = self.feature(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "net_lenet = my_lenet()\n",
    "print(net_lenet)\n",
    "    \n",
    "# 模型的可学习参数可以通过net.parameters()返回\n",
    "params = list(net_lenet.parameters())\n",
    "print(params[0])\n",
    "print(len(params))\n",
    "print(params[0].size())# conv1的权重\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2f5e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "# 前向传播\n",
    "out = net_lenet(input)\n",
    "torch.onnx.export(net_lenet, input, \"model_lenet.onnx\")\n",
    "\n",
    "#清零所有参数的梯度缓存，然后进行随机梯度的反向传播\n",
    "net_lenet.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715f87d",
   "metadata": {},
   "source": [
    "AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8705db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_alexnet(\n",
      "  (feature): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
      "    (4): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
      "    (8): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zcx/Apps/anaconda/envs/universe/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:314: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789259345/work/torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/zcx/Apps/anaconda/envs/universe/lib/python3.8/site-packages/torch/onnx/utils.py:739: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789259345/work/torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/zcx/Apps/anaconda/envs/universe/lib/python3.8/site-packages/torch/onnx/utils.py:1244: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789259345/work/torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"网络结构\n",
    "输入层：227×227×3的RGB图像\n",
    "层间结构（按顺序排列）：\n",
    "\n",
    "卷积层1\n",
    "卷积核：11×11，步长4，96个通道\n",
    "输出尺寸：55×55×96\n",
    "激活函数：ReLU\n",
    "池化层：3×3最大池化，步长2 → 输出27×27×96\n",
    "局部响应归一化（LRN）\n",
    "\n",
    "卷积层2\n",
    "卷积核：5×5，填充2，256个通道\n",
    "输出尺寸：27×27×256\n",
    "激活函数：ReLU\n",
    "池化层：3×3最大池化，步长2 → 输出13×13×256\n",
    "局部响应归一化（LRN）\n",
    "\n",
    "卷积层3\n",
    "卷积核：3×3，填充1，384个通道\n",
    "输出尺寸：13×13×384\n",
    "激活函数：ReLU\n",
    "\n",
    "卷积层4\n",
    "卷积核：3×3，填充1，384个通道\n",
    "输出尺寸：13×13×384\n",
    "激活函数：ReLU\n",
    "\n",
    "卷积层5\n",
    "卷积核：3×3，填充1，256个通道\n",
    "输出尺寸：13×13×256\n",
    "激活函数：ReLU\n",
    "池化层：3×3最大池化，步长2 → 输出6×6×256\n",
    "\n",
    "全连接层\n",
    "FC6：4096个神经元（输入维度：6×6×256=9216）\n",
    "Dropout率：0.5\n",
    "FC7：4096个神经元\n",
    "Dropout率：0.5\n",
    "FC8：1000个神经元（对应ImageNet类别）\n",
    "\"\"\"\n",
    "class my_alexnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4), # 卷积层1\n",
    "            nn.ReLU(), # 激活函数\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # 最大池化层\n",
    "            nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2), # 局部响应归一化\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2), # 卷积层2\n",
    "            nn.ReLU(), # 激活函数\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # 最大池化层\n",
    "            nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2), # 局部响应归一化\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1), # 卷积层3\n",
    "            nn.ReLU(), # 激活函数\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1), # 卷积层\n",
    "            nn.ReLU(), # 激活函数\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1), # 卷积层5\n",
    "            nn.ReLU(), # 激活函数\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # 最大池化层\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), # 展平层\n",
    "            nn.Linear(256 * 6 * 6, 4096), # 全连接层\n",
    "            #相比于LeNet，全连接层的输出个数大数倍。使用丢弃层来缓解过拟合\n",
    "            nn.Dropout(0.5), # Dropout层\n",
    "            nn.ReLU(), # 激活函数\n",
    "            nn.Linear(4096, 4096), # 全连接层\n",
    "            nn.Dropout(0.5), # Dropout层\n",
    "            nn.ReLU(), # 激活函数\n",
    "            nn.Linear(4096, 1000) # 全连接层\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "net_alexnet = my_alexnet()\n",
    "print(net_alexnet)\n",
    "input = torch.randn(1, 3, 227, 227)\n",
    "# 前向传播\n",
    "out = net_alexnet(input)\n",
    "torch.onnx.export(net_alexnet, input, \"model_alexnet.onnx\")\n",
    "# 清零所有参数的梯度缓存，然后进行随机梯度的反向传播\n",
    "net_alexnet.zero_grad()\n",
    "out.backward(torch.randn(1, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdd702",
   "metadata": {},
   "source": [
    "模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4742d7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntorch.nn.init提供了以下初始化方法： \\n1. torch.nn.init.uniform_(tensor, a=0.0, b=1.0) \\n2. torch.nn.init.normal_(tensor, mean=0.0, std=1.0) \\n3. torch.nn.init.constant_(tensor, val) \\n4. torch.nn.init.ones_(tensor) \\n5. torch.nn.init.zeros_(tensor) \\n6. torch.nn.init.eye_(tensor) \\n7. torch.nn.init.dirac_(tensor, groups=1) \\n8. torch.nn.init.xavier_uniform_(tensor, gain=1.0) \\n9. torch.nn.init.xavier_normal_(tensor, gain=1.0) \\n10. torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan__in', nonlinearity='leaky_relu') \\n11. torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu') \\n12. torch.nn.init.orthogonal_(tensor, gain=1) \\n13. torch.nn.init.sparse_(tensor, sparsity, std=0.01)\\n14. torch.nn.init.calculate_gain(nonlinearity, param=None)\\n\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.nn.init提供了以下初始化方法： \n",
    "1. torch.nn.init.uniform_(tensor, a=0.0, b=1.0) \n",
    "2. torch.nn.init.normal_(tensor, mean=0.0, std=1.0) \n",
    "3. torch.nn.init.constant_(tensor, val) \n",
    "4. torch.nn.init.ones_(tensor) \n",
    "5. torch.nn.init.zeros_(tensor) \n",
    "6. torch.nn.init.eye_(tensor) \n",
    "7. torch.nn.init.dirac_(tensor, groups=1) \n",
    "8. torch.nn.init.xavier_uniform_(tensor, gain=1.0) \n",
    "9. torch.nn.init.xavier_normal_(tensor, gain=1.0) \n",
    "10. torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan__in', nonlinearity='leaky_relu') \n",
    "11. torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu') \n",
    "12. torch.nn.init.orthogonal_(tensor, gain=1) \n",
    "13. torch.nn.init.sparse_(tensor, sparsity, std=0.01)\n",
    "14. torch.nn.init.calculate_gain(nonlinearity, param=None)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea4a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor([[[[ 0.1198,  0.1576,  0.3134],\n",
      "          [ 0.1904,  0.2399, -0.1122],\n",
      "          [-0.2739, -0.1738, -0.1941]]],\n",
      "\n",
      "\n",
      "        [[[-0.2303, -0.2003, -0.2437],\n",
      "          [ 0.2278, -0.0374, -0.2509],\n",
      "          [ 0.0449, -0.1777, -0.0911]]],\n",
      "\n",
      "\n",
      "        [[[-0.2894, -0.0926, -0.0481],\n",
      "          [-0.1950,  0.2164,  0.2436],\n",
      "          [ 0.3228, -0.1256,  0.2833]]]])\n",
      "tensor([[-0.2471,  0.0694, -0.1394, -0.2498, -0.2134, -0.2553, -0.3059, -0.1799,\n",
      "         -0.2508, -0.3075]])\n",
      "tensor([[[[-0.2425, -0.4311, -0.2917],\n",
      "          [ 0.3703,  0.3474,  0.2775],\n",
      "          [-0.0108,  0.3146, -0.3175]]],\n",
      "\n",
      "\n",
      "        [[[-0.2880,  0.5921, -0.0472],\n",
      "          [-0.7609,  1.0768,  0.7984],\n",
      "          [ 1.0630,  0.2771, -0.5665]]],\n",
      "\n",
      "\n",
      "        [[[-0.2564, -0.3100,  0.5399],\n",
      "          [ 0.2551,  0.2877, -0.5882],\n",
      "          [-0.3625, -0.3315,  0.1882]]]])\n",
      "tensor([[0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,\n",
      "         0.3000]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.nn.init中为我们提供了常用的初始化方法\n",
    "根据实际模型来使用torch.nn.init进行初始化，\n",
    "通常使用isinstance()来进行判断模块（回顾3.4模型构建）属于什么类型\n",
    "对于不同的类型层，可以设置不同的权值初始化的方法。\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "conv = nn.Conv2d(1,3,3)\n",
    "linear = nn.Linear(10,1)\n",
    "\n",
    "print(isinstance(conv,nn.Conv2d)) # 判断conv是否是nn.Conv2d类型\n",
    "print(isinstance(linear,nn.Conv2d)) # 判断linear是否是nn.Conv2d类型\n",
    "\n",
    "# Conv默认使用Kaiming均匀初始化（针对ReLU激活函数）\n",
    "# torch.nn.init.kaiming_uniform__(conv.weight.data, a=0, mode='fan_in', nonlinearity='relu')\n",
    "# Linear层默认使用PyTorch的默认初始化（类似均匀分布[-sqrt(1/in_features), sqrt(1/in_features)]）\n",
    "# torch.nn.init.uniform_(tensor, a=0.0, b=1.0) \n",
    "\n",
    "# 查看默认初始化的conv参数\n",
    "print(conv.weight.data)\n",
    "# 查看linear的参数\n",
    "print(linear.weight.data)\n",
    "\n",
    "# 使用torch.nn.init对conv进行kaiming初始化\n",
    "# Conv2d使用Kaiming正态分布初始化（均值为0，标准差由非线性函数类型决定）\n",
    "# Linear层使用固定常数0.3初始化\n",
    "torch.nn.init.kaiming_normal_(conv.weight.data)\n",
    "print(conv.weight.data)\n",
    "# 对linear进行常数初始化\n",
    "torch.nn.init.constant_(linear.weight.data,0.3)\n",
    "print(linear.weight.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee980e",
   "metadata": {},
   "source": [
    "初始化函数封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc411855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0696,  0.1003,  0.0408],\n",
      "          [ 0.0235, -0.0515, -0.2017],\n",
      "          [ 0.0274,  0.1809,  0.2459]]]])\n",
      "-------初始化-------\n",
      "tensor([[[[-0.5748, -0.3484,  0.7871],\n",
      "          [ 0.3020,  0.4273,  0.0608],\n",
      "          [ 0.1157,  0.2459,  0.3186]]]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "将各种初始化方法定义为一个initialize_weights()的函数并在模型初始后进行使用\n",
    "遍历当前模型的每一层，然后判断各层属于什么类型，然后根据不同类型层，设定不同的权值初始化方法\n",
    "在初始化时，最好不要将模型的参数初始化为0，因为这样会导致梯度消失，从而影响模型的训练效果。\n",
    "可以使用其他初始化方法或者将模型初始化为一个很小的值，如0.01，0.1等\n",
    "\"\"\"\n",
    "# 模型的定义\n",
    "class MLP(nn.Module):\n",
    "  # 声明带有模型参数的层，这里声明了两个全连接层\n",
    "  def __init__(self, **kwargs):\n",
    "    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\n",
    "    super(MLP, self).__init__(**kwargs)\n",
    "    self.hidden = nn.Conv2d(1,1,3)\n",
    "    self.act = nn.ReLU()\n",
    "    self.output = nn.Linear(10,1)\n",
    "    \n",
    "   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\n",
    "  def forward(self, x):\n",
    "    o = self.act(self.hidden(x))\n",
    "    return self.output(o)\n",
    "\n",
    "mlp = MLP()\n",
    "print(mlp.hidden.weight.data)\n",
    "print(\"-------初始化-------\")\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_uniform_(m.weight.data, a=0, mode='fan_in', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.uniform_(m.weight.data, a=-0.1, b=0.1)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.uniform_(m.weight.data, a=0.1, b=0.1)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# 初始化自定义模型参数\n",
    "initialize_weights(mlp)\n",
    "print(mlp.hidden.weight.data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
