{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f26d24",
   "metadata": {},
   "source": [
    "损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PyTorch中常用的损失函数\n",
    "# 计算二分类任务时的交叉熵（Cross Entropy）函数\n",
    "1. torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "# 交叉熵函数\n",
    "2. torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
    "# L1损失函数,计算输出y和真实标签target之间的差值的绝对值\n",
    "3. torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n",
    "# MSE损失函数,计算输出y和真实标签target之间的差值的平方\n",
    "4. torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "# 平滑L1 (Smooth L1)损失函数， L1的平滑输出，其功能是减轻离群点带来的影响\n",
    "5. torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=1.0)\n",
    "# 目标泊松分布的负对数似然损失，泊松分布的负对数似然损失函数\n",
    "6. torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')\n",
    "# KL散度，也就是计算相对熵。用于连续分布的距离度量，并且对离散采用的连续输出空间分布进行回归通常很有用。\n",
    "7. torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)\n",
    "# MarginRankingLoss，计算两个向量之间的相似度，用于排序任务。该方法用于计算两组数据之间的差异。\n",
    "8. torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n",
    "# 多标签边界损失函数，对于多标签分类问题计算损失函数\n",
    "9.torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')\n",
    "# 二分类损失函数, 二分类的 logistic 损失。\n",
    "10.torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')\n",
    "# 多分类的折页损失\n",
    "11. torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "# 三元组损失,希望去anchor的距离更接近positive examples，而远离negative examples\n",
    "12. torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n",
    "# HingEmbeddingLoss, 对输出的embedding结果做Hing损失计算\n",
    "13. torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')\n",
    "# 余弦相似度，将余弦相似度作为一个距离的计算方式，如果两个向量的距离近，则损失函数值小，反之亦然\n",
    "14. torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n",
    "# CTC损失函数，用于解决时序类数据的分类\n",
    "15. torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8468e028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([-0.2444, -0.0517,  0.6541], requires_grad=True)\n",
      "target: tensor([1., 1., 1.])\n",
      "output: tensor(0.6536, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# 使用BCELoss计算二分类任务的交叉熵损失\n",
    "# 注意：输入需要经过Sigmoid激活函数处理\n",
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "print(\"input:\", input)\n",
    "target = torch.empty(3).random_(2)\n",
    "print(\"target:\", target)\n",
    "output = loss(m(input), target)\n",
    "print(\"BCELoss损失函数的计算结果为:\", output)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c02eaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[-0.4884,  0.1553, -0.1917,  0.8103,  0.4447],\n",
      "        [ 0.7099, -1.3003,  0.6739,  0.0164,  1.5402],\n",
      "        [ 1.0833, -1.1766, -2.2679,  1.7250,  0.5305]], requires_grad=True)\n",
      "target: tensor([4, 0, 1])\n",
      "output: tensor(2.1822, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(\"input:\", input)\n",
    "# 注意：输入不需要经过Softmax激活函数处理\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(\"target:\", target)\n",
    "output = loss(input, target)\n",
    "print(\"交叉熵损失函数的计算结果为:\", output)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d367a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[-0.5901, -0.2502,  0.1925, -2.0331, -0.9004],\n",
      "        [-0.9269, -1.3695,  0.1490, -1.0374,  1.3361],\n",
      "        [ 1.8951,  2.4225, -0.2311, -0.4141,  0.0485]], requires_grad=True)\n",
      "target: tensor([[ 1.7490, -0.3514, -1.1049, -1.2197,  0.7158],\n",
      "        [-0.4237,  0.0408, -0.9130, -1.2334, -0.2821],\n",
      "        [-0.5107, -0.2122,  1.2934,  0.8623, -0.4861]])\n",
      "output: tensor(1.2889, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.L1Loss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(\"input:\", input)\n",
    "target = torch.randn(3, 5)\n",
    "print(\"target:\", target)\n",
    "output = loss(input, target)\n",
    "print(\"'L1损失函数的计算结果为:\", output)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41702e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[ 1.3246, -1.3853, -0.0266,  1.2004, -1.0868],\n",
      "        [ 1.3070, -0.3433,  0.1438, -0.8947, -0.3764],\n",
      "        [ 0.0554, -0.1030, -0.6538,  0.7210, -0.1273]], requires_grad=True)\n",
      "target: tensor([[ 1.3370,  2.7128, -0.3621,  1.7593,  0.4754],\n",
      "        [-0.7533, -0.5846,  0.1655, -0.1042, -0.5822],\n",
      "        [-1.1543, -0.1282, -1.7099, -0.1448,  0.4791]])\n",
      "MSE损失函数的计算结果为 tensor(1.8885, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(\"input:\", input)\n",
    "target = torch.randn(3, 5)\n",
    "print(\"target:\", target)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('MSE损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf0a34",
   "metadata": {},
   "source": [
    "训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DataLoader构建完成后，训练过程\n",
    "for data, label in train_loader: # for循环读取DataLoader中的全部数据\n",
    "    data, label = data.cuda(), label.cuda() # 数据放到GPU上\n",
    "    optimizer.zero_grad() # 当前批次数据做训练时，应当先将优化器的梯度置零\n",
    "    output = model(data) # 将数据输入到模型中，得到输出\n",
    "    loss = criterion(output, label) # 计算损失函数\n",
    "    loss.backward() # 反向传播计算梯度\n",
    "    optimizer.step() # 更新模型参数\n",
    "\n",
    "验证/测试的流程基本与训练过程一致，不同点在于：\n",
    "需要预先设置torch.no_grad，以及将model调至eval模式\n",
    "不需要将优化器的梯度置零\n",
    "不需要将loss反向回传到网络\n",
    "不需要更新optimizer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd7e61",
   "metadata": {},
   "source": [
    "pytorch优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0255b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "深度学习的目标是通过不断改变网络参数，使得参数能够对输入做各种非线性变换拟合输出\n",
    "本质上是一个函数去寻找最优解，只不过这个最优解是一个矩阵，而如何快速求得这个最优解是深度学习研究的一个重点\n",
    "为了使求解参数过程更快，暴力求解可实施性为0，采用BP+优化器逼近求解\n",
    "优化器是根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值，使得模型输出更加接近真实标签\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d638fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "优化器的库torch.optim\n",
    "常用的优化器有：\n",
    "torch.optim.SGD\n",
    "torch.optim.ASGD\n",
    "torch.optim.Adadelta\n",
    "torch.optim.Adagrad\n",
    "torch.optim.Adam\n",
    "torch.optim.AdamW\n",
    "torch.optim.Adamax\n",
    "torch.optim.RAdam\n",
    "torch.optim.NAdam\n",
    "torch.optim.SparseAdam\n",
    "torch.optim.LBFGS\n",
    "torch.optim.RMSprop\n",
    "torch.optim.Rprop\n",
    "\n",
    "均继承于Optimizer\n",
    "class Optimizer(object):\n",
    "    def __init__(self, params, defaults):        \n",
    "        self.defaults = defaults\n",
    "        self.state = defaultdict(dict)\n",
    "        self.param_groups = []\n",
    "\n",
    "属性：\n",
    "defaults：存储的是优化器的超参数\n",
    "state：参数的缓存\n",
    "param_groups：管理的参数组，是一个list，其中每个元素是一个字典，顺序是params，lr，momentum，dampening，weight_decay，nesterov\n",
    "方法：\n",
    "zero_grad()：清空所管理参数的梯度，PyTorch的特性是张量的梯度不自动清零，因此每次反向传播后都需要清空梯度。\n",
    "step()：执行一步梯度更新，参数更新\n",
    "add_param_group()：添加参数组\n",
    "load_state_dict() ：加载状态参数字典，可以用来进行模型的断点续训练，继续上次的参数进行训练\n",
    "state_dict()：获取优化器当前状态信息字典\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37de74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data of weight before step:\n",
      "tensor([[1.5941, 0.9487],\n",
      "        [0.1566, 1.5587]])\n",
      "The grad of weight before step:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "The data of weight after step:\n",
      "tensor([[1.4941, 0.8487],\n",
      "        [0.0566, 1.4587]])\n",
      "The grad of weight after step:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "The grad of weight after optimizer.zero_grad():\n",
      "None\n",
      "optimizer.params_group is \n",
      "[{'params': [tensor([[1.4941, 0.8487],\n",
      "        [0.0566, 1.4587]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}]\n",
      "weight in optimizer:139920104718720\n",
      "weight in weight:139920104718720\n",
      "\n",
      "optimizer.param_groups is\n",
      "[{'params': [tensor([[1.4941, 0.8487],\n",
      "        [0.0566, 1.4587]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}, {'params': [tensor([[-0.2377,  0.0679,  1.7692],\n",
      "        [ 0.4091,  0.2568, -1.1405],\n",
      "        [-0.4556,  1.2097, -0.0702]], requires_grad=True)], 'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}]\n",
      "state_dict before step:\n",
      " {'state': {0: {'momentum_buffer': tensor([[1., 1.],\n",
      "        [1., 1.]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [1]}]}\n",
      "state_dict after step:\n",
      " {'state': {0: {'momentum_buffer': tensor([[1., 1.],\n",
      "        [1., 1.]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [1]}]}\n",
      "----------done-----------\n",
      "load state_dict successfully\n",
      "{'state': {0: {'momentum_buffer': tensor([[1., 1.],\n",
      "        [1., 1.]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [1]}]}\n",
      "\n",
      "{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}\n",
      "\n",
      "defaultdict(<class 'dict'>, {tensor([[1.4941, 0.8487],\n",
      "        [0.0566, 1.4587]], requires_grad=True): {'momentum_buffer': tensor([[1., 1.],\n",
      "        [1., 1.]])}})\n",
      "\n",
      "[{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [tensor([[1.4941, 0.8487],\n",
      "        [0.0566, 1.4587]], requires_grad=True)]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [tensor([[-0.2377,  0.0679,  1.7692],\n",
      "        [ 0.4091,  0.2568, -1.1405],\n",
      "        [-0.4556,  1.2097, -0.0702]], requires_grad=True)]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3133035/3709693578.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"optimizer_state_dict.pkl\") # 需要修改为你自己的路径\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 设置权重，服从正态分布  --> 2 x 2\n",
    "weight = torch.randn((2, 2), requires_grad=True)\n",
    "# 设置梯度为全1矩阵  --> 2 x 2\n",
    "weight.grad = torch.ones((2, 2))\n",
    "# 输出现有的weight和data\n",
    "print(\"The data of weight before step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight before step:\\n{}\".format(weight.grad))\n",
    "# 实例化优化器\n",
    "optimizer = torch.optim.SGD([weight], lr=0.1, momentum=0.9)\n",
    "# 进行一步操作\n",
    "optimizer.step()\n",
    "# 查看进行一步后的值，梯度\n",
    "print(\"The data of weight after step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight after step:\\n{}\".format(weight.grad))\n",
    "# 权重清零\n",
    "optimizer.zero_grad()\n",
    "# 检验权重是否为0\n",
    "print(\"The grad of weight after optimizer.zero_grad():\\n{}\".format(weight.grad))\n",
    "# 输出参数\n",
    "print(\"optimizer.params_group is \\n{}\".format(optimizer.param_groups))\n",
    "# 查看参数位置，optimizer和weight的位置一样\n",
    "print(\"weight in optimizer:{}\\nweight in weight:{}\\n\".format(id(optimizer.param_groups[0]['params'][0]), id(weight)))\n",
    "\n",
    "# 添加参数：weight2\n",
    "weight2 = torch.randn((3, 3), requires_grad=True)\n",
    "optimizer.add_param_group({\"params\": weight2, 'lr': 0.0001, 'nesterov': True})\n",
    "# 查看现有的参数\n",
    "print(\"optimizer.param_groups is\\n{}\".format(optimizer.param_groups))\n",
    "# 查看当前状态信息\n",
    "opt_state_dict = optimizer.state_dict()\n",
    "print(\"state_dict before step:\\n\", opt_state_dict)\n",
    "# 进行5次step操作\n",
    "for _ in range(50):\n",
    "    optimizer.step()\n",
    "# 输出现有状态信息\n",
    "print(\"state_dict after step:\\n\", optimizer.state_dict())\n",
    "# 保存参数信息\n",
    "torch.save(optimizer.state_dict(),\"optimizer_state_dict.pkl\")\n",
    "print(\"----------done-----------\")\n",
    "# 加载参数信息\n",
    "state_dict = torch.load(\"optimizer_state_dict.pkl\")\n",
    "optimizer.load_state_dict(state_dict)\n",
    "print(\"load state_dict successfully\\n{}\".format(state_dict))\n",
    "# 输出最后属性信息\n",
    "print(\"\\n{}\".format(optimizer.defaults))\n",
    "print(\"\\n{}\".format(optimizer.state))\n",
    "print(\"\\n{}\".format(optimizer.param_groups))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
