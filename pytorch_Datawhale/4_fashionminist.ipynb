{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e75d4e",
   "metadata": {},
   "source": [
    "导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5b74978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f55ed2",
   "metadata": {},
   "source": [
    "配置环境和超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ff21c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "lr_rate = 1e-4\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa96578",
   "metadata": {},
   "source": [
    "数据读入和加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c875011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "两种方式\n",
    "1.下载并使用PyTorch提供的内置数据集\n",
    "2.从网站下载以csv格式存储的数据，读入并转成预期的格式\n",
    "torchvision包可以方便对图像数据进行处理变换,需要将数据格式转为Tensor类\n",
    "\"\"\"\n",
    "from torchvision import transforms\n",
    "\n",
    "image_size = 28\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 采用自行构建Dataset类的方式\n",
    "class FMDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.images = df.iloc[:, 1:].values.astype(np.uint8)\n",
    "        self.labels = df.iloc[:, 0].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape(28, 28, 1)\n",
    "        label = int(self.labels[idx])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.tensor(image/255.,dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "train_df = pd.read_csv(\"/home/zcx/Data/work/pytorch/data/fashion-mnist_train.csv\")\n",
    "test_df = pd.read_csv(\"/home/zcx/Data/work/pytorch/data/fashion-mnist_test.csv\")\n",
    "train_dataset = FMDataset(train_df, transform=data_transform)\n",
    "test_dataset = FMDataset(test_df, transform=data_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe94e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义DataLoader类，以便在训练和测试时加载数据\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,num_workers=num_workers, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddcb8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 1, 28, 28])\n",
      "Label shape: torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7facdf417520>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 做一些数据可视化操作，主要是验证我们读入的数据是否正确\n",
    "image, label = next(iter(train_loader))\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Label shape:\", label.shape)\n",
    "plt.imshow(image[0][0], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b53ed1",
   "metadata": {},
   "source": [
    "手搭CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d719273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 64 * 4 * 4)  # 展平\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873d86a",
   "metadata": {},
   "source": [
    "训练测试和验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90d38516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}')\n",
    "\n",
    "def val(epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    gt_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            gt_labels.append(target.cpu().data.numpy())\n",
    "            pred_labels.append(preds.cpu().data.numpy())\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "    gt_labels = np.concatenate(gt_labels)\n",
    "    pred_labels = np.concatenate(pred_labels)\n",
    "    accuracy = np.mean(gt_labels == pred_labels)\n",
    "    print(f'Epoch {epoch}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.5064\n",
      "Epoch 1, Val Loss: 0.3402, Accuracy: 0.8754\n",
      "Epoch 2, Train Loss: 0.3457\n",
      "Epoch 2, Val Loss: 0.2881, Accuracy: 0.8923\n",
      "Epoch 3, Train Loss: 0.3054\n",
      "Epoch 3, Val Loss: 0.2649, Accuracy: 0.9007\n",
      "Epoch 4, Train Loss: 0.2849\n",
      "Epoch 4, Val Loss: 0.2459, Accuracy: 0.9079\n",
      "Epoch 5, Train Loss: 0.2690\n",
      "Epoch 5, Val Loss: 0.2437, Accuracy: 0.9093\n",
      "Epoch 6, Train Loss: 0.2591\n",
      "Epoch 6, Val Loss: 0.2414, Accuracy: 0.9158\n",
      "Epoch 7, Train Loss: 0.2498\n",
      "Epoch 7, Val Loss: 0.2340, Accuracy: 0.9109\n",
      "Epoch 8, Train Loss: 0.2433\n",
      "Epoch 8, Val Loss: 0.2595, Accuracy: 0.9089\n",
      "Epoch 9, Train Loss: 0.2387\n",
      "Epoch 9, Val Loss: 0.2438, Accuracy: 0.9130\n",
      "Epoch 10, Train Loss: 0.2310\n",
      "Epoch 10, Val Loss: 0.2335, Accuracy: 0.9152\n",
      "Epoch 11, Train Loss: 0.2329\n",
      "Epoch 11, Val Loss: 0.2301, Accuracy: 0.9197\n",
      "Epoch 12, Train Loss: 0.2297\n",
      "Epoch 12, Val Loss: 0.2317, Accuracy: 0.9176\n",
      "Epoch 13, Train Loss: 0.2265\n",
      "Epoch 13, Val Loss: 0.2316, Accuracy: 0.9192\n",
      "Epoch 14, Train Loss: 0.2250\n",
      "Epoch 14, Val Loss: 0.2293, Accuracy: 0.9186\n",
      "Epoch 15, Train Loss: 0.2217\n",
      "Epoch 15, Val Loss: 0.2273, Accuracy: 0.9200\n",
      "Epoch 16, Train Loss: 0.2213\n",
      "Epoch 16, Val Loss: 0.2381, Accuracy: 0.9183\n",
      "Epoch 17, Train Loss: 0.2181\n",
      "Epoch 17, Val Loss: 0.2416, Accuracy: 0.9167\n",
      "Epoch 18, Train Loss: 0.2176\n",
      "Epoch 18, Val Loss: 0.2384, Accuracy: 0.9189\n",
      "Epoch 19, Train Loss: 0.2165\n",
      "Epoch 19, Val Loss: 0.2241, Accuracy: 0.9192\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(epoch)\n",
    "    val(epoch)\n",
    "\n",
    "# 保存模型\n",
    "model_path = \"/home/zcx/Data/work/pytorch/model/fashion_mnist/fashion_mnist_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
